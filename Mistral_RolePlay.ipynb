{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM2M2dHLGchl5u3UJ43lLUH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0x11c11e/the-art-of-fine-tuning/blob/main/Mistral_RolePlay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPYdSg02V-Hp"
      },
      "outputs": [],
      "source": [
        "! pip install transformers\n",
        "! pip install datasets\n",
        "! pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from the specified repository\n",
        "dataset = load_dataset(\"iamketan25/roleplay-instructions-dataset\", split='train')\n",
        "\n",
        "# Perform a train-test split to separate 10% of the data for evaluation\n",
        "train_test_split = dataset.train_test_split(test_size=0.1) # 10% of the dataset\n",
        "\n",
        "# Extract the training and evaluation datasets from the split\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n"
      ],
      "metadata": {
        "id": "tWLohXglWM4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0])"
      ],
      "metadata": {
        "id": "g5z5OTIPgy-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer for the specified pretrained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "\n",
        "# Ensure the tokenizer utilizes the end-of-sequence token as the padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenizes the input examples using the pretrained tokenizer.\n",
        "\n",
        "    Parameters:\n",
        "    - examples: A dictionary containing the input prompts and the corresponding chosen responses.\n",
        "\n",
        "    Returns:\n",
        "    A dictionary of tokenized model inputs and labels, suitable for training a machine learning model.\n",
        "    \"\"\"\n",
        "    # Tokenize the prompts with specified options\n",
        "    model_inputs = tokenizer(examples['prompt'], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "    # Tokenize the chosen responses to use as labels, ensuring consistent tokenization settings\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples['chosen'], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "    # Append the tokenized labels to the model inputs\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenize the training and evaluation datasets, removing original columns to streamline the data structure\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
        "tokenized_eval = eval_dataset.map(tokenize_function, batched=True, remove_columns=eval_dataset.column_names)\n"
      ],
      "metadata": {
        "id": "6HCqDzobWbnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load the pre-trained causal language model from the specified source\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "\n",
        "# Note: It is assumed that the corresponding tokenizer has been loaded previously in the workflow.\n",
        "# The tokenizer is essential for pre-processing the input data before feeding it to the model."
      ],
      "metadata": {
        "id": "amEyNKoNyl9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "\n",
        "# Initialize the model with a pre-trained causal language model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "\n",
        "# Configure the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',  # Directory where the model predictions and checkpoints will be stored\n",
        "    num_train_epochs=3,  # Total number of training epochs\n",
        "    per_device_train_batch_size=4,  # Batch size per device during training\n",
        "    per_device_eval_batch_size=8,  # Batch size for evaluation\n",
        "    warmup_steps=500,  # Number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,  # Weight decay if we apply some.\n",
        "    logging_dir='./logs',  # Directory for storing logs\n",
        "    logging_steps=10,  # Log every X updates steps.\n",
        "    evaluation_strategy=\"epoch\",  # Evaluation is done at the end of each epoch\n",
        "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,  # The instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,  # Training arguments, defined above\n",
        "    train_dataset=tokenized_train,  # Training dataset\n",
        "    eval_dataset=tokenized_eval,  # Evaluation dataset\n",
        ")\n",
        "\n",
        "# Note: The `tokenized_train` and `tokenized_eval` datasets should be prepared beforehand,\n",
        "# following the tokenization process suitable for your model and data.\n"
      ],
      "metadata": {
        "id": "b3kyL7P5XWlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the model training\n",
        "trainer.train()\n",
        "\n",
        "# After training, to visualize and monitor the training process using TensorBoard,\n",
        "# the following Jupyter notebook magic commands are used:\n",
        "\n",
        "# Load the TensorBoard notebook extension. This line is only needed once per notebook.\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Launch TensorBoard within the notebook to visualize metrics.\n",
        "# The log directory is specified to where the training logs are saved.\n",
        "%tensorboard --logdir ./logs\n"
      ],
      "metadata": {
        "id": "xxZTIugDXXSc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}